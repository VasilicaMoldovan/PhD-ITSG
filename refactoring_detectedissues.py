# -*- coding: utf-8 -*-
"""refactoring_detectedIssues.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/182_4lpJNk9iUO-fqaM4TMcgwwjam8QDR
"""

pip install lazypredict

!pip install lightgbm

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
import joblib
import ast

from sklearn.model_selection import train_test_split, KFold, cross_validate
from sklearn.linear_model import Perceptron
from lazypredict.Supervised import LazyClassifier
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
import plotly.express as px
from sklearn import preprocessing
from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, make_scorer, accuracy_score, recall_score, precision_score, f1_score
from sklearn import metrics
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from lightgbm import LGBMClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from collections import Counter

from google.colab import drive
drive.mount('/content/gdrive')

df_uims = pd.read_excel('/content/gdrive/MyDrive/maintainability/Refs/JEditRefactoringsAndTDOneRefOneComp.xlsx')
df_uims.drop(columns=df_uims.columns[0], axis=1,  inplace=True)

df_uims.head(10)

df_uims['RTypes'] = df_uims['RTypes'].replace('[]', 'empty_list')

label_encoder = LabelEncoder()
df_uims['RTypes'] = label_encoder.fit_transform(df_uims['RTypes'])

train, test = train_test_split(df_uims, test_size=0.2)

features = df_uims.columns[:-1]
X = train[features] # her we are droping the output feature as this is the target and 'X' is input features, the changes are not
                                # made inplace as we have not used 'inplace = True'
y = train['RTypes'] # Output/Dependent variable
XTest = test[features]
YTest = test['RTypes']

X_train, X_val_, y_train, y_val_ = train_test_split(X, y, test_size = 0.3, random_state = 42)

X_val, X_test, y_val, y_test = train_test_split(XTest, YTest, test_size = 0.5, random_state = 42)

# lets print the shapes again
print("Shape of the X Train :", X_train.shape)
print("Shape of the y Train :", y_train.shape)
print("Shape of the X val :", X_val.shape)
print("Shape of the y val :", y_val.shape)
print("Shape of the X test :", X_test.shape)
print("Shape of the y test :", y_test.shape)

fs = SelectKBest(score_func=f_classif, k='all')

print(X_train)

fs.fit(X_train, y_train)

feature_names=X.columns
for i in range(len(fs.scores_)):
	print('Importance of ' +feature_names[i]+' is %f' % (fs.scores_[i]))
# plot the scores
plt.rcParams["figure.figsize"] = (8,8)
plt.bar([i for i in X.columns], fs.scores_)
plt.show()

reg = LazyClassifier(predictions=True)
models, predictions = reg.fit(X_train, X_test, y_train, y_test)

models

clf = LazyClassifier(predictions=True)

train, test = train_test_split(df, test_size=0.2)

features = df.columns[:-1]
features
X = train[features] # her we are droping the output feature as this is the target and 'X' is input features, the changes are not
                                # made inplace as we have not used 'inplace = True'
y = train['Frequency'] # Output/Dependent variable
XTest = test[features]
YTest = test['Frequency']

X_train, X_val_, y_train, y_val_ = train_test_split(X, y, test_size = 0.3, random_state = 42)

gb = GradientBoostingClassifier(
        n_estimators=200,
        validation_fraction=0.2,
        n_iter_no_change=5,
        tol=0.01,
        random_state=0,
    )

gb.fit(X_train, y_train)

X_val, X_test, y_val, y_test = train_test_split(XTest, YTest, test_size = 0.5, random_state = 42)

scores = gb.score(X_test, y_test)
scores

models, predictions = clf.fit(X_train, X_test, y_train, y_test)
models

model = ExtraTreesClassifier()

scoring = {
    'accuracy': make_scorer(accuracy_score),
    'recall': make_scorer(recall_score, average='weighted'),
    'precision': make_scorer(precision_score, average='weighted'),
    'f1_score': make_scorer(f1_score, average='weighted')
}

results = cross_validate(model, X, y, cv=5, scoring=scoring)

# Access the results
accuracy_scores = results['test_accuracy']
recall_scores = results['test_recall']
precision_scores = results['test_precision']
f1_scores = results['test_f1_score']

print(accuracy_scores)
print(recall_scores)
print(precision_scores)
print(f1_scores)

print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))

model.fit(X_train, y_train)

y_pred_test= model.predict(X_test)

print(classification_report(np.array(y_test), y_pred_test))

df_test = pd.read_excel('/content/gdrive/MyDrive/maintainability/try.xlsx')

df_test.head()

df_test.drop(columns=df_test.columns[0], axis=1,  inplace=True)

df_test.head()

y_pred_test= model.predict(X_test)

fs = SelectKBest(score_func=f_classif, k='all')

fs.fit(X_test, y_test)

feature_names=X.columns
for i in range(len(fs.scores_)):
	print('Importance of ' +feature_names[i]+' is %f' % (fs.scores_[i]))
# plot the scores
plt.rcParams["figure.figsize"] = (8,8)
plt.bar([i for i in X.columns], fs.scores_)
plt.show()

print(classification_report(np.array(y_test), y_pred_test))

cm = confusion_matrix(y_test, y_pred_test)
print(cm)

filename = '/content/gdrive/MyDrive/maintainability/Refs/finalized_model.sav'
joblib.dump(model, filename)

df = pd.read_excel('/content/gdrive/MyDrive/maintainability/Refs/TuxGuitarRefactoringsAndTDOneRefOneComp.xlsx')
df.drop(columns=df.columns[0], axis=1,  inplace=True)

df = df.dropna()

classes_to_increase = df['RTypes'].value_counts()[df['RTypes'].value_counts() == 1].index.tolist()

# Create a mask to identify rows corresponding to classes_to_increase
mask = df['RTypes'].isin(classes_to_increase)

# Separate data into X and y
X = df.drop(columns=['RTypes'])
y = df['RTypes']

# Apply over-sampling only to the specified classes
oversampler = RandomOverSampler(sampling_strategy={cls: 3500 for cls in classes_to_increase})
X_resampled, y_resampled = oversampler.fit_resample(X[mask], y[mask])

# Combine the resampled data with the original data for the specified classes
df_resampled = pd.concat([X_resampled, y_resampled], axis=1)

# Combine the resampled data with the original data for other classes
df_final = pd.concat([df[~mask], df_resampled])

df = df_final

le = LabelEncoder()
#df['severity'] = le.fit_transform(df['severity'])
#df['type'] = le.fit_transform(df['type'])
df['RTypes'] = le.fit_transform(df['RTypes'])

train, test = train_test_split(df, test_size=0.2)

features = df.columns[:-1]
features
X = train[features] # her we are droping the output feature as this is the target and 'X' is input features, the changes are not
                                # made inplace as we have not used 'inplace = True'
y = train['RTypes'] # Output/Dependent variable
XTest = test[features]
YTest = test['RTypes']

X = np.asarray(X).astype('float32')
y = np.asarray(y).astype('float32')

#norms = np.linalg.norm(X, axis=1, keepdims=True)
#X = X / norms
min_val = 0.0
max_val = 36.0

# Check if values are outside the interval and set them to 0.0
X[(X < min_val) | (X > max_val)] = 0.0

X = np.nan_to_num(X, nan=0.0)

print(set(df['RTypes']))

print(Counter(y))

int_array = y.astype(int)
predicted_labels = label_encoder.inverse_transform(int_array)
print(Counter(predicted_labels))

sampling_strategy = {0: 1500, 1: 1600, 2: 1800, 3: 1500, 4: 1500, 7: 1500, 8: 1500, 9: 1500, 10: 1500, 11: 1500, 12: 1500, 13: 1500, 14: 1500, 15: 1500, 16: 1500, 17: 1500}
over = SMOTE(sampling_strategy=sampling_strategy)
under = RandomUnderSampler(sampling_strategy=sampling_strategy)
steps = [('o', over), ('u', under)]
pipeline = Pipeline(steps=steps)
X, y = pipeline.fit_resample(X, y)

clf = LazyClassifier(predictions=True)

X_train, X_val_, y_train, y_val_ = train_test_split(X, y, test_size = 0.3, random_state = 42)

gb = GradientBoostingClassifier(
        n_estimators=200,
        validation_fraction=0.2,
        n_iter_no_change=5,
        tol=0.01,
        random_state=0,
    )

gb.fit(X_train, y_train)

X_val, X_test, y_val, y_test = train_test_split(XTest, YTest, test_size = 0.5, random_state = 42)

scores = gb.score(X_test, y_test)
scores

models, predictions = clf.fit(X_train, X_test, y_train, y_test)
models

model = ExtraTreesClassifier()

scoring = {
    'accuracy': make_scorer(accuracy_score),
    'recall': make_scorer(recall_score, average='weighted'),
    'precision': make_scorer(precision_score, average='weighted'),
    'f1_score': make_scorer(f1_score, average='weighted')
}

results = cross_validate(model, X, y, cv=5, scoring=scoring)

# Access the results
accuracy_scores = results['test_accuracy']
recall_scores = results['test_recall']
precision_scores = results['test_precision']
f1_scores = results['test_f1_score']

print(accuracy_scores)
print(recall_scores)
print(precision_scores)
print(f1_scores)

model.fit(X_train, y_train)

y_pred_test= model.predict(X_test)

print(classification_report(np.array(y_test), y_pred_test))

print(type(y_pred_test[0]))

int_array = y_pred_test.astype(int)
predicted_labels = label_encoder.inverse_transform(int_array)

print(predicted_labels)

filename = '/content/gdrive/MyDrive/maintainability/Refs/finalized_model.sav'
joblib.dump(model, filename)

filename = '/content/gdrive/MyDrive/maintainability/encoder.sav'
joblib.dump(label_encoder, filename)